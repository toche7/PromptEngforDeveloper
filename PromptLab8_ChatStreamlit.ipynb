{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toche7/PromptEngforDeveloper/blob/main/PromptLab8_ChatStreamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLFov09h18yC"
      },
      "source": [
        "# Lab Chatbot with Streamlit\n",
        "เอกสารนี้พัฒนามาจากเอกสารของ Jeff Heaton, McKelvey School of Engineering, Washington University in St. Louis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ใน Lab นี้ เราจะได้สร้างแอปแชท LLM โดยใช้ StreamLit อย่างเป็นขั้นเป็นตอน เพื่อให้ทุกคนเข้าถึงและทำตามได้ง่าย โดยจะรันแอปนี้ผ่าน Google Colab เพื่อให้ใช้งานได้สะดวกขึ้น อีกทั้งเราจะพาไปรู้จักกับ `llm_util.py` ซึ่งเป็นสคริปต์ยูทิลิตี้ที่ช่วยให้ทำงานกับโมเดลภาษาขนาดใหญ่ (LLMs) ที่รองรับ LangChain ได้ง่ายยิ่งขึ้น ในตัวอย่างนี้ เราจะใช้ LLM ของ Meta เป็นตัวขับเคลื่อนสำหรับแอปแชทของเรา เมื่อจบโมดูลนี้ คุณจะได้แอปแชทที่ใช้งานได้จริง พร้อมความเข้าใจพื้นฐานในการผสาน LLM เข้าไปในโปรเจกต์ต่าง ๆ โดยใช้ StreamLit\n",
        "\n",
        "โค้ดต่อไปนี้เริ่มต้นด้วยการนำเข้าไลบรารี `os` เพื่อจัดการกับระบบไฟล์และตัวแปรสภาพแวดล้อม จากนั้นตรวจสอบว่าโค้ดรันใน Google Colab หรือไม่ โดยพยายามนำเข้าโมดูลจาก `google.colab` ถ้านำเข้าได้ จะตั้งค่าตัวแปร `COLAB` เป็น `True` และพิมพ์ข้อความแจ้งว่ากำลังใช้ Colab แต่ถ้าไม่สามารถนำเข้าได้ จะตั้งค่าเป็น `False` แทน\n",
        "\n",
        "ถ้าอยู่ใน Colab จะตั้งค่าตัวแปรสภาพแวดล้อม `GROQ_API_KEY` จากข้อมูลที่เก็บไว้ใน Colab แล้วติดตั้งไลบรารีที่จำเป็นสำหรับการทำงาน เช่น `langchain`, `langchain_openai`, `openai`, `streamlit`, และ `langchain-groq` ผ่านคำสั่ง pip โดยรวมโค้ดนี้ทำให้สามารถใช้งานได้อย่างเหมาะสมทั้งใน Google Colab และสภาพแวดล้อมอื่น ๆ"
      ],
      "metadata": {
        "id": "L2fUJk5P1gPn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWGARRT92DrA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    from google.colab import drive, userdata\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "# Groq Secrets\n",
        "if COLAB:\n",
        "    os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# Install needed libraries in CoLab\n",
        "if COLAB:\n",
        "    !pip install langchain langchain_openai openai streamlit langchain-groq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2MPPX0c1pHi"
      },
      "source": [
        "# Creating a Chat Application\n",
        "\n",
        "เราจะสร้างไฟล์ดังนี้:\n",
        "\n",
        "* **app.py** - แอปพลิเคชันหลักของ StreamLit สำหรับแชท\n",
        "\n",
        "\n",
        "## Chat Application\n",
        "โค้ดนี้สร้างแอปพลิเคชันแชทบอทโดยใช้ Streamlit และโมเดลภาษาจาก `ChatGroq` ในไลบรารี LangChain โดยการตั้งค่าเริ่มต้นและรูปแบบการทำงานจะอยู่ในไฟล์ชื่อ `app.py` โดยโค้ดมีรายละเอียดดังนี้:\n",
        "\n",
        "1. **การนำเข้าโมดูล**:\n",
        "   - นำเข้า `ChatGroq` จาก `langchain_groq` สำหรับการทำงานกับโมเดลภาษา\n",
        "   - นำเข้า Streamlit (`st`) เพื่อสร้างอินเทอร์เฟซผู้ใช้\n",
        "   - นำเข้า `ConversationSummaryMemory`, `PromptTemplate`, และ `ConversationChain` จาก LangChain ซึ่งใช้สำหรับจัดการหน่วยความจำ, การสร้างเทมเพลตคำถาม, และจัดการการสนทนา\n",
        "\n",
        "2. **การตั้งค่าโมเดลภาษา (LLM)**:\n",
        "   - โค้ดกำหนดตัวแปร `llm` เป็นอินสแตนซ์ของ `ChatGroq` และระบุโมเดลภาษาที่จะใช้คือ `\"gemma2-9b-it\"`\n",
        "\n",
        "3. **ฟังก์ชัน `create_chatbot`**:\n",
        "   - ฟังก์ชันนี้ใช้สร้างแชทบอทที่มีหน่วยความจำ (`ConversationSummaryMemory`) ซึ่งช่วยให้แชทบอทสามารถติดตามบริบทของการสนทนา\n",
        "   - มีการกำหนด `template` สำหรับแชทบอท ซึ่งเริ่มต้นด้วยข้อความของระบบว่า “System: You are a helpful assistant. Answer questions clearly and concisely.” เพื่อให้แชทบอททำงานตามบทบาทที่กำหนดไว้\n",
        "   - ตัวเทมเพลตประกอบด้วย `{history}` สำหรับประวัติการสนทนาและ `{input}` สำหรับข้อความที่ผู้ใช้ป้อน จากนั้นคืนค่าเป็น `ConversationChain` ที่ประกอบไปด้วยโมเดลภาษา, เทมเพลต, และหน่วยความจำ เพื่อให้แชทบอทสามารถใช้ข้อมูลบริบทจากการสนทนาเดิมได้\n",
        "\n",
        "4. **การตั้งค่า Streamlit**:\n",
        "   - แสดงหัวข้อแอปเป็น \"Chat\" โดยใช้ `st.title(\"Chat\")`\n",
        "   - เช็คว่าใน `st.session_state` มีอินสแตนซ์ของแชทบอทหรือยัง ถ้ายังไม่มี จะสร้างแชทบอทใหม่โดยเรียก `create_chatbot`\n",
        "   - เช็คสถานะของ `st.session_state.messages` เพื่อเก็บประวัติการสนทนา ถ้ายังไม่มีจะสร้างเป็นลิสต์ว่าง ๆ\n",
        "\n",
        "5. **การแสดงประวัติการสนทนา**:\n",
        "   - ลูปผ่าน `st.session_state.messages` และแสดงข้อความจากผู้ใช้และแชทบอททีละข้อความบนหน้าจอแชท\n",
        "\n",
        "6. **การรับข้อความจากผู้ใช้และตอบกลับ**:\n",
        "   - เมื่อผู้ใช้กรอกข้อความในช่องอินพุต ข้อความนั้นจะถูกเพิ่มเข้าในประวัติการสนทนาและแสดงในอินเทอร์เฟซ\n",
        "   - แชทบอทประมวลผลข้อความที่ได้รับโดยใช้ฟังก์ชัน `predict` ของ `ConversationChain` และสร้างข้อความตอบกลับ จากนั้นแสดงในส่วนของผู้ช่วยและบันทึกลงในประวัติการสนทนา\n",
        "\n",
        "โดยรวม โค้ดนี้สร้างแอปพลิเคชันแชทบอทที่มีระบบหน่วยความจำและบริบทเพื่อให้การสนทนาสอดคล้องและต่อเนื่อง\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEQ9c5Akqj_R"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "from langchain_groq import ChatGroq\n",
        "import streamlit as st\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chains import ConversationChain\n",
        "import sys\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"gemma2-9b-it\",\n",
        ")\n",
        "\n",
        "def create_chatbot(llm):\n",
        "    memory = ConversationSummaryMemory(llm=llm)\n",
        "\n",
        "    # Define the system prompt directly in the template\n",
        "    template = \"\"\"System: You are a helpful assistant. Answer questions clearly and concisely.\n",
        "\n",
        "{history}\n",
        "User: {input}\n",
        "\n",
        "Assistant:\n",
        "\"\"\"\n",
        "    PROMPT = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
        "    return ConversationChain(llm=llm, prompt=PROMPT, memory=memory, verbose=False)\n",
        "\n",
        "\n",
        "st.title(\"Chat\")\n",
        "\n",
        "# Initialize the chat and messages\n",
        "if \"chat\" not in st.session_state:\n",
        "    st.session_state.chat = create_chatbot(llm)\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display the conversation history\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(\n",
        "            message[\"content\"],\n",
        "        )\n",
        "\n",
        "# Process user input and chatbot response\n",
        "if prompt := st.chat_input(\"What is up?\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(\n",
        "            prompt,\n",
        "        )\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        response = st.session_state.chat.predict(input=prompt)\n",
        "        st.markdown(\n",
        "            response,\n",
        "        )\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMqVgvmxhySA"
      },
      "source": [
        "เราต้องไปเอา password จากการใช้ lib localtunnel เพื่อการเข้าถึงการใช้งาน server จากภายนอกเพื่อทำให้ run Streamlit ใน Colab ได้  \n",
        "\n",
        "ตัวเลข IP ที่ได้นี้จะเป็น password ที่เราจะนำไปใช้ใน step ต่อไป"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iJmAPR8c-JE"
      },
      "outputs": [],
      "source": [
        "!curl https://loca.lt/mytunnelpassword"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3lKrMcvh3_F"
      },
      "source": [
        "We launch the StreamLit server and obtain its URL. You will need the above password when you access the URL it gives you.\n",
        "\n",
        "เราจะทำการขึ้น StreamLit Server และได้ URL ของมัน กดไปที่ link ที่ได้จากการ run คำสั่งด้านล่างนี้"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSLPrByztQOe"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt &\n",
        "!npx --yes localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "คำสั่งนี้เป็นการใช้ใน Google Colab เพื่อรันแอปพลิเคชัน Streamlit และเชื่อมต่อกับ LocalTunnel ดังนี้:\n",
        "\n",
        "1. **รันแอป Streamlit**:\n",
        "   - `!streamlit run app.py &>/content/logs.txt &`\n",
        "   - คำสั่งนี้จะรันไฟล์ Python ที่ชื่อ `app.py` โดยใช้ Streamlit\n",
        "   - `&>` ใช้เพื่อส่งออกผลลัพธ์ทั้ง stdout และ stderr ไปยังไฟล์ `logs.txt` ในโฟลเดอร์ `/content`\n",
        "   - `&` ที่ท้ายคำสั่งหมายความว่าการรันจะเกิดขึ้นในพื้นหลัง (background) ทำให้คุณสามารถรันคำสั่งถัดไปได้โดยไม่ต้องรอให้ Streamlit เสร็จสิ้น\n",
        "\n",
        "2. **เชื่อมต่อกับ LocalTunnel**:\n",
        "   - `!npx --yes localtunnel --port 8501`\n",
        "   - คำสั่งนี้จะใช้ `npx` เพื่อเรียกใช้งาน LocalTunnel ซึ่งจะทำการสร้าง URL สาธารณะเพื่อเข้าถึงแอป Streamlit ที่รันอยู่ที่พอร์ต 8501\n",
        "   - `--yes` จะยืนยันการติดตั้งแพ็คเกจที่จำเป็น\n",
        "\n"
      ],
      "metadata": {
        "id": "I2oL6qr66LVA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUH52Q_wknk0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}